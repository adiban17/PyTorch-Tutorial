{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXjk8lA4jUWaX5DoFGgBz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adiban17/PyTorch-Tutorial/blob/main/01_PyTorch_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk2h52VRY2h0",
        "outputId": "c50f8bef-7571-4118-fd52-62623459453a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to PyTorch workflow !\n"
          ]
        }
      ],
      "source": [
        "print(\"Welcome to PyTorch workflow !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Workflow\n",
        "Lets's explore an example PyTorch workflow"
      ],
      "metadata": {
        "id": "GxU3RaZBZHGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "what_were_covering = {1:\"data (prepare and load)\",\n",
        "                      2:\"build model\",\n",
        "                      3:\"fitting the model to data (training)\",\n",
        "                      4:\"making predictions and evaluating a model (inference)\",\n",
        "                      5:\"saving and loading a model\",\n",
        "                      6:\"putting it all together\"}\n",
        "what_were_covering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSbca_4ZFz5",
        "outputId": "94480713-38d1-4075-c012-7d00062b17d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'data (prepare and load)',\n",
              " 2: 'build model',\n",
              " 3: 'fitting the model to data (training)',\n",
              " 4: 'making predictions and evaluating a model (inference)',\n",
              " 5: 'saving and loading a model',\n",
              " 6: 'putting it all together'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn  # nn contains all of PyTorch's building blocks for neural networks\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YFXHjouIZ5Mv",
        "outputId": "3ab5c41a-eea3-480e-9d95-80b9fb65f896"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data (preparing and loading)\n",
        "Data can be anything... in machine learning.\n",
        "* Excel spreadsheet\n",
        "* Images of any kind\n",
        "* Videos (YouTube has lots of data)\n",
        "* DNA\n",
        "* Text\n",
        "\n",
        "Machine learning is a game of two parts:\n",
        "1. Get data into a numerical representation.\n",
        "2. Build a model to learn patterns in that numerical representation.\n",
        "\n",
        "To showcase this, let's create some known data using the linear regression formula.\n",
        "We'll use a linear regression formula to make a straight line with known **parameters**."
      ],
      "metadata": {
        "id": "9wtFWkxJc61x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create known parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# create\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight*X + bias\n",
        "X[:10],y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udv-3GFmczbQ",
        "outputId": "ecc33394-3dc8-4c2a-ed8e-c7333198ed00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDyzjxwLeiM0",
        "outputId": "2a649ad0-6862-4dac-8549-c3752c0a28ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into train and test splits (one of the most important concepts in machine learning in general)\n",
        "\n",
        "Let's create a training and test set with our data.\n"
      ],
      "metadata": {
        "id": "i5xXDGPLe-Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a train/test split\n",
        "train_split = int(0.8*len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOqmylaoe9FN",
        "outputId": "6ffed830-2698-45a7-de57-954df989cbe7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How might we better visualize our data ?\n",
        "\n",
        "This is where the data explorer's motto comes in !\n",
        "\n",
        "\"Visualize, visualize, visualize!\""
      ],
      "metadata": {
        "id": "1iLPaO3oghVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10,7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing Data\")\n",
        "\n",
        "  # Are there predictions ?\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions if they exist\n",
        "    plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\":14})"
      ],
      "metadata": {
        "id": "FIeRUvFCgNi2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "VlTCaynFiEX_",
        "outputId": "f11e9db0-b8dd-4754-addc-05ea8acde813"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJGCAYAAACTJvC6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASwxJREFUeJzt3X98U/Xd//9nGmgKg5bxq/yqFN1E3BAUpAN0JLNaNy9OmG6iTkCmbjjUXem8EKZS0GndNWVsEX/MoTjdBlPRnGv4ZY6a4tA6NpBNFOqU34UWmJoiSgvp+f6RD6lZW2hK2yQnj/vtltsZJ+fkvBJOWZ++33m/HJZlWQIAAAAAG8lIdAEAAAAA0N4IOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHa6JLqA1mhoaNDevXvVs2dPORyORJcDAAAAIEEsy9KhQ4c0aNAgZWS0PG6TEkFn7969ysvLS3QZAAAAAJLE7t27NWTIkBafT4mg07NnT0mRN5OdnZ3gagAAAAAkSm1trfLy8qIZoSUpEXSOT1fLzs4m6AAAAAA46VdaWIwAAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYTkosL90WR48eVTgcTnQZQEI5nU517do10WUAAAB0OtsFndraWh08eFB1dXWJLgVICi6XS3379qUHFQAASCtxB51XX31VP/vZz7Rhwwbt27dPL7zwgqZMmXLCc8rLy1VcXKy3335beXl5uvPOO3Xddde1seSW1dbWqqqqSj169FDfvn3VtWvXkzYSAuzKsiwdPXpUoVBIVVVVkkTYAQAAaSPuoHP48GGNGjVK3/3ud3X55Zef9Pjt27frsssu06xZs/Tb3/5WZWVluuGGGzRw4EAVFRW1qeiWHDx4UD169NCQIUMIOICkbt26qWfPntqzZ48OHjxI0AEAAGkj7qDz9a9/XV//+tdbffyjjz6qYcOG6cEHH5QkjRgxQuvWrdPPf/7zdg06R48eVV1dnfr27UvIAT7D4XAoJydHVVVVOnr0KN/ZAQAAaaHDV12rqKhQYWFhzL6ioiJVVFS0eE5dXZ1qa2tjHidzfOEBfokDmjr+c8ECHQAAIF10eNCprq5Wbm5uzL7c3FzV1tbq008/bfac0tJS5eTkRB95eXmtvh6jOUBT/FwAAIB0k5R9dObNm6dQKBR97N69O9ElAQAAAEghHb689IABA1RTUxOzr6amRtnZ2erWrVuz57hcLrlcro4uDQAAAIBNdfiIzvjx41VWVhaz789//rPGjx/f0ZdGJ3E4HHK73af0GuXl5XI4HFqwYEG71AQAAID0FnfQ+fjjj7Vp0yZt2rRJUmT56E2bNmnXrl2SItPOpk+fHj1+1qxZ2rZtm+bMmaOtW7fq4Ycf1h/+8Af5fL72eQeQFAkb8Txwcvn5+TGfmcvlUr9+/TRu3DjNnj1b69ata5frEPIAAADaX9xT1/7+97/L4/FE/1xcXCxJmjFjhpYtW6Z9+/ZFQ48kDRs2TKtWrZLP59MvfvELDRkyRL/+9a/bvYdOuispKWmyb/HixQqFQs0+1562bNmi7t27n9JrjBs3Tlu2bFHfvn3bqar24XQ6deedd0qSjh07pg8//FBvvfWWHnvsMT388MOaPHmynnrqKX3+859PcKUAAAD4LIdlWVaiiziZ2tpa5eTkKBQKtdjw8MiRI9q+fbuGDRumrKysTq4wOeXn52vnzp1Kgb/ipJSfn6/q6modOXKkyXM7d+7U9ddfr7KyMk2aNEmvvPKKMjLaNhO0vLxcHo9HJSUlHTaqw88HAACwi9ZkAylJV11Dx9mxY4ccDoeuu+46bdmyRd/85jfVp08fORwO7dixQ5L0wgsv6Oqrr9YXvvAFde/eXTk5Obrwwgv1/PPPN/uazX1H57rrrpPD4dD27dv1y1/+UmeddZZcLpeGDh2qhQsXqqGhIeb4lqZv5efnKz8/Xx9//LF++MMfatCgQXK5XDrnnHP03HPPtfgep06dqt69e6tHjx6aNGmSXn31VS1YsEAOh0Pl5eVt+ehiDB06VP/3f/+nESNGaO3atU1qeeKJJ+T1epWfn6+srCz17t1bRUVFCgaDMcctWLAgOkK6cOHCmKlyx/8+3n33Xc2ZM0fnnXee+vTpo6ysLJ155pmaO3euPv7441N+LwAAAHbU4auuITm99957+spXvqKRI0fquuuu07///W9lZmZKinzPKjMzUxdccIEGDhyoAwcOyDRNfetb39Ivf/lL3XLLLa2+zv/8z/9o7dq1+q//+i8VFRXpxRdf1IIFC1RfX6977723Va9x9OhRXXLJJfrwww91xRVX6JNPPtHy5ct15ZVXavXq1brkkkuix1ZVVWnChAnat2+fLr30Up177rmqrKzUxRdfrK997WvxfUgn0a1bN9122226/vrrtWLFCl155ZXR52bPnq1Ro0apsLBQ/fr1U1VVlV588UUVFhZq5cqV8nq9kiS3260dO3boqaee0qRJk2ICY69evSRJK1eu1NKlS+XxeOR2u9XQ0KA33nhDP/3pT7V27Vq9+uqrNMoFAAAdxqw0FdwelGeYR8ZwI9HltJ6VAkKhkCXJCoVCLR7z6aefWu+884716aefdmJlyW3o0KHWf/4Vb9++3ZJkSbLmz5/f7Hnvv/9+k32HDh2yRo4caeXk5FiHDx+OeU6SNWnSpJh9M2bMsCRZw4YNs/bu3Rvdf+DAAatXr15Wz549rbq6uuj+YDBoSbJKSkqafQ9erzfm+DVr1liSrKKiopjjr732WkuSde+998bsX7p0afR9B4PBZt/3fxo6dKjlcrlOeMz7779vSbLy8vJi9m/btq3JsXv37rUGDRpkffGLX4zZ39J7P27Pnj0x7/24hQsXWpKsZ5555iTvhJ8PAADQNoGtAUsLZDkXOi0tkBXYGkh0Sa3KBpZlWUxdS1MDBgzQHXfc0exzp59+epN9PXr00HXXXadQKKS//e1vrb7OXXfdpYEDB0b/3LdvX3m9Xh06dEiVlZWtfp2f//zn0REnSbrooos0dOjQmFrq6ur07LPPqn///vrRj34Uc/7MmTM1fPjwVl+vtQYNGiRJOnjwYMz+YcOGNTl24MCBuuKKK/Svf/1LO3fubPU1Bg8eHPPej7v55pslSWvWrImnZAAAgFYLbg/K6XAqbIXldDhVvqM80SW1GkGnjUxT8vki21Q0atSoZn95lqT9+/eruLhYI0aMUPfu3aPfGTkeHvbu3dvq64wZM6bJviFDhkiSPvroo1a9Rq9evZoNDkOGDIl5jcrKStXV1Wns2LFNGs46HA5NmDCh1XWfqm3btunGG2/UGWecoaysrOhn6Pf7JcX3GVqWpSeeeEJf/epX1bt3bzmdTjkcDvXp0yfu1wIAAIiHZ5gnGnLCVljufHeiS2o1vqPTBqYpeb2S0yktXiwFApKRQtMVJSk3N7fZ/R988IHOP/987dq1SxMnTlRhYaF69eolp9OpTZs2KRAIqK6urtXXaW4ljC5dIrddOBxu1Wvk5OQ0u79Lly4xixrU1tZKkvr379/s8S2951NxPGT069cvuu+9997TuHHjVFtbK4/Ho8mTJys7O1sZGRkqLy/X2rVr4/oMb731Vj300EPKy8uTYRgaOHBgNMgtXLgwrtcCAACIhzHcUOCqgMp3lMud706p7+gQdNogGIyEnHA4si0vT72g01LT0KVLl2rXrl265557ov1jjrv//vsVCAQ6o7w2OR6q9u/f3+zzNTU17X7N4yu4nX/++dF9P//5z/Xhhx/q6aef1rXXXhtz/KxZs7R27dpWv/7+/fu1ZMkSnXPOOaqoqIjpV1RdXa2FCxee2hsAAAA4CWO4kVIB5zimrrWBx9MYcsJh6T9WVk5p77//viRFVwX7rL/85S+dXU5chg8fLpfLpQ0bNjQZ5bAsSxUVFe16vU8//VQPPvigJOnqq6+O7m/pM7QsS6+99lqT13E6nZKaH+Hatm2bLMtSYWFhk6asyf73AQAAkEgEnTYwjMh0tVtvTc1paycydOhQSdK6deti9v/ud7/TSy+9lIiSWs3lculb3/qWampqtHjx4pjnfvOb32jr1q3tdq1du3Zp8uTJeuedd+TxeHT55ZdHn2vpM7z//vu1efPmJq/Vu3dvSdLu3bubPHf8tV5//fWYaXp79uzRvHnzTv2NAAAA2BRT19rIMOwVcI6bNm2afvrTn+qWW25RMBjU0KFD9Y9//ENlZWW6/PLLtXLlykSXeEKlpaVas2aN5s6dq7Vr10b76Pzxj3/UpZdeqtWrVysjo/X5/tixY9EmpuFwWB999JH++c9/6rXXXlM4HJbX69WyZctipgLOmjVLTz75pK644gpdeeWV6tOnj9544w1t3LhRl112mVatWhVzjbPOOkuDBg3S8uXL5XK5NGTIEDkcDt1yyy3Rldqef/55jR07VhdddJFqamr0xz/+URdddFF09AgAAACxCDqIMWTIEK1du1Zz5szRmjVrdOzYMZ133nl6+eWXtXv37qQPOnl5eaqoqNDtt9+ul19+WWvXrtWYMWP08ssv69lnn5XU/AIJLQmHw9HvwWRmZio7O1vDhg3T97//fV1zzTWaOHFik3POPfdcvfzyy7rzzju1cuVKOZ1OTZgwQa+99ppM02wSdJxOp1auXKnbb79dv//973Xo0CFJ0rXXXqucnBwtW7ZM+fn5ev755+X3+3XaaaepuLhYt99+u5577rm2flQAAAC25rAsy0p0ESdTW1urnJwchUKhFn9JPXLkiLZv365hw4YpKyurkytEKrjgggtUUVGhUCikHj16JLqcTsXPBwAAMCtNBbcH5RnmScnFBY5rTTaQ+I4ObGjfvn1N9j3zzDN67bXXVFhYmHYhBwAAwKw05V3ulX+9X97lXpmVKdoMMg5MXYPtfPnLX9a5556rs88+O9r/p7y8XD179tQDDzyQ6PIAAAA6XXB7MNr00+lwqnxHeUqP6rQGIzqwnVmzZmn//v36zW9+o4ceekiVlZW65pprtH79eo0cOTLR5QEAAHQ6zzBPNOSErbDc+e5El9Th+I4OkAb4+QAAAGalqfId5XLnu1N6NKe139Fh6hoAAACQBozhRkoHnHgxdQ0AAACA7RB0AAAAANgOQQcAAACA7RB0AAAAANgOQQcAAABIIWalKd9qX1o0/TwVBB0AAAAgRZiVprzLvfKv98u73EvYOQGCDgAAAJAigtuD0aafTodT5TvKE11S0iLoAAAAACnCM8wTDTlhKyx3vjvRJSUtGoaiU7jdbq1du1aWZSW6FAAAgJRlDDcUuCqg8h3lcue706oBaLwY0bEJh8MR16O9LViwQA6HQ+Xl5e3+2h1h2bJlMZ9HRkaGsrOzNWzYMHm9Xvn9fn3wwQftci23290hnzkAAEhPxnBDi4oWEXJOghEdmygpKWmyb/HixQqFQs0+19l+85vf6JNPPkl0GU1cdNFFuuCCCyRJH3/8saqqqvSXv/xFpmmqpKREjz32mL797W8nuEoAAADEi6BjEwsWLGiyb9myZQqFQs0+19lOO+20RJfQrMLCQs2dOzdmXzgc1lNPPaWbb75ZV199tXJycnTJJZckqEIAAAC0BVPX0lB9fb0WLVqk8847T5/73OfUs2dPXXjhhTLNpssThkIhzZ8/X2effbZ69Oih7OxsfeELX9CMGTO0c+dOSZGpWQsXLpQkeTye6HSw/Pz86Os0N33r+PSxZcuW6eWXX9aECRPUvXt39enTRzNmzNC///3vZut/7LHH9KUvfUlZWVnKy8vTnDlzdOTIETkcDrnd7lP+fJxOp7773e/qkUceUTgcVnFxccx3i959913NmTNH5513nvr06aOsrCydeeaZmjt3rj7++OOY13I4HFq7dm30fx9/XHfdddFjnnjiCXm9XuXn5ysrK0u9e/dWUVGRgsHgKb8XAACAdMWITpqpq6vTpZdeqvLyco0ePVrXX3+9jh49qlWrVkW/m3LzzTdLkizLUlFRkf76179q4sSJuvTSS5WRkaGdO3fKNE1NmzZNQ4cOjf7SvnbtWs2YMSMacHr16tWqmkzT1KpVqzR58mRNmDBBr776qn7zm9/o/fff17p162KOnT9/vu655x7l5ubqxhtvVNeuXfWHP/xBW7duba+PKGratGkqKSnR22+/rc2bN2vkyJGSpJUrV2rp0qXyeDxyu91qaGjQG2+8oZ/+9Kdau3atXn31VXXt2lVSZErhsmXLtHPnzpgphKNHj47+79mzZ2vUqFEqLCxUv379VFVVpRdffFGFhYVauXKlvF5vu783AAAA27NSQCgUsiRZoVCoxWM+/fRT65133rE+/fTTTqwsuQ0dOtT6z7/iH//4x5Yk66677rIaGhqi+2tra62xY8damZmZVlVVlWVZlvXPf/7TkmRNmTKlyWsfOXLEOnToUPTPJSUlliQrGAw2W8ukSZOa1PLkk09akqwuXbpY69ati+4/duyY5Xa7LUlWRUVFdH9lZaXldDqtwYMHWzU1NTG1n3322ZYka9KkSSf/YD5z7dLS0hMeN23aNEuStXTp0ui+PXv2WHV1dU2OXbhwoSXJeuaZZ0763j9r27ZtTfbt3bvXGjRokPXFL37xZG+lVfj5AAAguQS2Bqz//v/+2wpsDSS6lJTTmmxgWZbF1LU2MitN+Vb7UqobbUNDgx555BGdccYZWrhwYcxUsp49e2r+/Pmqr6/XypUrY87r1q1bk9dyuVzq0aNHu9R1zTXXaOLEidE/O51OzZgxQ5L0t7/9Lbr/97//vcLhsH70ox+pf//+MbXfeeed7VLLfxo0aJAk6eDBg9F9gwcPVmZmZpNjj4+ErVmzJq5rDBs2rMm+gQMH6oorrtC//vWv6BRBAABgD2alKe9yr/zr/fIu96bU75OphKlrbXD85nQ6nFr818UKXBVIieX9Kisr9eGHH2rQoEHR79R81oEDByQpOg1sxIgROuecc/T73/9ee/bs0ZQpU+R2uzV69GhlZLRfRh4zZkyTfUOGDJEkffTRR9F9//jHPyQpukraZ302KHU0y7L05JNPatmyZdq8ebNCoZAaGhqiz+/duzeu19u2bZtKS0v1yiuvqKqqSnV1dTHP7927V0OHDm2X2gEAQOIFtwejDT+dDqfKd5SnxO+SqYag0wapenMe7wvz9ttv6+23327xuMOHD0uSunTpoldeeUULFizQ888/rx/96EeSpH79+unmm2/WHXfcIafTecp1ZWdnN9nXpUvk1gyHw9F9tbW1khQzmnNcbm7uKdfRnOOhpV+/ftF9t956qx566CHl5eXJMAwNHDhQLpdLkrRw4cImQeVE3nvvPY0bN061tbXyeDyaPHmysrOzlZGRofLycq1duzau1wMAAMnPM8yjxX9dHP190p3vTnRJtkTQaYNUvTmPB4orrrhCzz33XKvO6dOnj/x+v375y19q69ateuWVV+T3+1VSUqKuXbtq3rx5HVlyjOP179+/v8kIR01NTbtfr6GhQa+++qok6fzzz49ee8mSJTrnnHNUUVGh7t27R4+vrq5udqTsRH7+85/rww8/1NNPP61rr7025rlZs2ZFV2wDAAD2YQw3FLgqoPId5XLnu1PiP5inIr6j0wbHb85bC25NmWlrUmQqWnZ2tv7+97/r6NGjcZ3rcDg0YsQIzZ49W3/+858lKWY56uMjO58dgWlvo0aNkiS99tprTZ57/fXX2/16Tz/9tHbu3KmRI0fqS1/6kqTINDPLslRYWBgTciTpL3/5S7Ovc6LP5v3335ekJiurWZbV7PsEAAD2YAw3tKhoUcr8HpmKCDptlIo3Z5cuXXTTTTdp586duu2225oNO5s3b9b+/fslSTt27NCOHTuaHHN89CQrKyu6r3fv3pKk3bt3d0DlEVdddZUyMjL04IMPxiwOcPjwYd17773tdp1wOKwnn3xSN910k5xOpxYtWhRduOH4SNLrr78e872cPXv2tDi6daLP5vjr/ecy2vfff782b9586m8GAAAgTTF1Lc0sXLhQGzdu1C9/+UutWrVKX/3qV9W/f39VVVXprbfe0j/+8Q9VVFSof//+2rRpky6//HKNGzdOZ599tgYMGBDt8ZKRkSGfzxd93eONQn/84x/r7bffVk5Ojnr16hVdiaw9DB8+XHPnztV9992nkSNH6sorr1SXLl20cuVKjRw5Ups3b457kYQ1a9boyJEjkqRPPvlEe/bs0auvvqqqqir17t1bTz/9tAoLC6PHH18N7fnnn9fYsWN10UUXqaamRn/84x910UUXRUdoPutrX/uannvuOV1xxRX6+te/rqysLI0aNUqTJ0/WrFmz9OSTT+qKK67QlVdeqT59+uiNN97Qxo0bddlll2nVqlWn9qEBAACkq85Y6/pU0UenbZrro2NZkT41jz32mDVx4kQrOzvbcrlc1mmnnWZdeuml1iOPPGJ9/PHHlmVZ1u7du625c+daX/nKV6z+/ftbmZmZ1mmnnWZdfvnlMf1tjlu2bJk1cuRIy+VyWZKsoUOHRp87UR+dJ598sslrBYNBS5JVUlLS5LmHH37YGjFihJWZmWkNGTLEuu2226zdu3dbkiyv19uqz+b4tY8/HA6H1aNHDys/P9+aPHmy5ff7rQ8++KDZcw8dOmT96Ec/svLz8y2Xy2V98YtftO655x6rvr6+2V4+R48etebMmWOddtppVpcuXSxJ1owZM2Le68SJE62ePXtavXr1sr7xjW9YGzZsOGlvonjw8wEAAOyitX10HJZlWYkIWPGora1VTk6OQqFQsyt0SdKRI0e0fft2DRs2LGZKFdLDmjVrdPHFF2vOnDn66U9/muhykg4/HwAAwC5akw0kvqODFHPgwIEmX+r/6KOPot+PmTJlSgKqAgAA6SoVm8inC76jg5Ty29/+Vg888IC+9rWvadCgQdq3b59Wr16t/fv367rrrtP48eMTXSIAAEgTqdpEPl0QdJBSJkyYoDFjxmjNmjX64IMP5HQ6NWLECN111136wQ9+kOjyAABAGknVJvLpgqCDlDJu3DgFAoFElwEAAJCyTeTTBUEHAAAAaIPjTeTLd5TLne9mNCfJEHQAAACANjKGGwScJGW7VddSYLVsoNPxcwEAANKNbYKO0+mUJB09ejTBlQDJ5/jPxfGfEwAAALuzTdDp2rWrXC6XQqEQ//Ua+AzLshQKheRyudS1a9dElwMAANApbPUdnb59+6qqqkp79uxRTk6OunbtKofDkeiygISwLEtHjx5VKBTSxx9/rMGDBye6JAAAgE5jq6CTnZ0tSTp48KCqqqoSXA2QHFwulwYPHhz9+QAAAE2ZlaaC24PyDPOwuIBNOKwUmOdVW1urnJwchUKhVv+ydvToUYXD4Q6uDEhuTqeT6WoAAJyEWWnKu9wb7YcTuCpA2Elirc0GthrR+ayuXbvyCx4AAABOKrg9GA05TodT5TvKCTo2YJvFCAAAAIC28AzzRENO2ArLne9OdEloB7Yd0QEAAABawxhuKHBVQOU7yuXOdzOaYxO2/Y4OAAAAAPtpbTZg6hoAAAAA2yHoAAAAALAdgg4AAAAA22lT0FmyZIny8/OVlZWlgoICrV+/vsVjjx49qrvvvltnnHGGsrKyNGrUKK1evbrNBQMAAADAycQddFasWKHi4mKVlJRo48aNGjVqlIqKirR///5mj7/zzjv12GOPye/365133tGsWbP0zW9+U2+++eYpFw8AAAAcZ1aa8q32yaw0E10KkkDcq64VFBTo/PPP10MPPSRJamhoUF5enm655RbNnTu3yfGDBg3SHXfcodmzZ0f3XXHFFerWrZueeeaZVl2TVdcAAABwImalKe9yb7QXTuCqAMtE21SHrLpWX1+vDRs2qLCwsPEFMjJUWFioioqKZs+pq6tTVlZWzL5u3bpp3bp1LV6nrq5OtbW1MQ8AAACgJcHtwWjIcTqcKt9RnuiSkGBxBZ2DBw8qHA4rNzc3Zn9ubq6qq6ubPaeoqEiLFi3Sv/71LzU0NOjPf/6zVq5cqX379rV4ndLSUuXk5EQfeXl58ZQJAACANOMZ5omGnLAVljvfneiSkGAdvuraL37xC33xi1/UWWedpczMTN18882aOXOmMjJavvS8efMUCoWij927d3d0mQAAAEhhxnBDgasCurXgVqatQZLUJZ6D+/btK6fTqZqampj9NTU1GjBgQLPn9OvXTy+++KKOHDmif//73xo0aJDmzp2r008/vcXruFwuuVyueEoDAABAmjOGGwQcRMU1opOZmakxY8aorKwsuq+hoUFlZWUaP378Cc/NysrS4MGDdezYMT3//PPyer1tqxgAAAAATiKuER1JKi4u1owZMzR27FiNGzdOixcv1uHDhzVz5kxJ0vTp0zV48GCVlpZKkv7617+qqqpKo0ePVlVVlRYsWKCGhgbNmTOnfd8JAAAAAPw/cQedqVOn6sCBA5o/f76qq6s1evRorV69OrpAwa5du2K+f3PkyBHdeeed2rZtm3r06KFvfOMbevrpp9WrV692exMAAAAA8Flx99FJBProAAAAAJA6qI8OAAAA0NHMSlO+1T6ZlWaiS0EKI+gAAAAgaZiVprzLvfKv98u73EvYQZsRdAAAAJA0gtuD0aafTodT5TvKE10SUhRBBwAAAEnDM8wTDTlhKyx3vjvRJSFFxb3qGgAAANBRjOGGAlcFVL6jXO58Nw1A0WasugYAAAAgZbDqGgAAAIC0RdABAAAAYDsEHQAAAAC2Q9ABAAAAYDsEHQAAALQ7s9KUb7WPhp9IGIIOAAAA2pVZacq73Cv/er+8y72EHSQEQQcAAADtKrg9GG346XQ4Vb6jPNElIQ0RdAAAANCuPMM80ZATtsJy57sTXRLSUJdEFwAAAAB7MYYbClwVUPmOcrnz3TKGG4kuCWnIYVmWlegiTqa13U8BAAAA2FtrswFT1wAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAANAis9KUb7WPpp9IOQQdAAAANMusNOVd7pV/vV/e5V7CDlIKQQcAAADNCm4PRpt+Oh1Ole8oT3RJQKsRdAAAANAszzBPNOSErbDc+e5ElwS0WpdEFwAAAIDkZAw3FLgqoPId5XLnu2UMNxJdEtBqDsuyrEQXcTKt7X4KAAAAwN5amw2YugYAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAJAGTFPy+SJbIB0QdAAAAGzONCWvV/L7I1vCDtIBQQcAAMDmgkHJ6ZTC4ci2vDzRFQEdj6ADAABgcx5PY8gJhyW3O9EVAR2vS6ILAAAAQMcyDCkQiIzkuN2RPwN2R9ABAABIA4ZBwEF6YeoaAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAABAijBNyeej4SfQGgQdAACAFGCaktcr+f2RLWEHODGCDgAAQAoIBhsbfjqdkZ44AFpG0AEAAEgBHk9jyAmHI40/AbSMhqEAAAApwDCkQCAykuN20/wTOBmCDgAAQIowDAIO0FpMXQMAAABgOwQdAAAAALZD0AEAAABgOwQdAAAAALZD0AEAAOhkpin5fDT9BDoSQQcAAKATmabk9Up+f2RL2AE6BkEHAACgEwWDjU0/nc5IXxwA7Y+gAwAA0Ik8nsaQEw5Hmn8CaH80DAUAAOhEhiEFApGRHLebBqBARyHoAAAAdDLDIOAAHY2pawAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAG1kmpLPR9NPIBm1KegsWbJE+fn5ysrKUkFBgdavX3/C4xcvXqzhw4erW7duysvLk8/n05EjR9pUMAAAQDIwTcnrlfz+yJawAySXuIPOihUrVFxcrJKSEm3cuFGjRo1SUVGR9u/f3+zxv/vd7zR37lyVlJRoy5YtWrp0qVasWKEf//jHp1w8AABAogSDjU0/nc5IXxwAySPuoLNo0SLdeOONmjlzps4++2w9+uij6t69u5544olmj3/99dc1ceJEXXPNNcrPz9cll1yiq6+++qSjQAAAAMnM42kMOeFwpPkngOQRV9Cpr6/Xhg0bVFhY2PgCGRkqLCxURUVFs+dMmDBBGzZsiAabbdu26aWXXtI3vvGNFq9TV1en2tramAcAAEAyMQwpEJBuvTWypQEokFy6xHPwwYMHFQ6HlZubG7M/NzdXW7dubfaca665RgcPHtQFF1wgy7J07NgxzZo164RT10pLS7Vw4cJ4SgMAAOh0hkHAAZJVh6+6Vl5ervvuu08PP/ywNm7cqJUrV2rVqlW65557Wjxn3rx5CoVC0cfu3bs7ukwAAAAANhLXiE7fvn3ldDpVU1MTs7+mpkYDBgxo9py77rpL06ZN0w033CBJGjlypA4fPqzvfe97uuOOO5SR0TRruVwuuVyueEoDAAAAgKi4RnQyMzM1ZswYlZWVRfc1NDSorKxM48ePb/acTz75pEmYcTqdkiTLsuKtFwAAAABOKq4RHUkqLi7WjBkzNHbsWI0bN06LFy/W4cOHNXPmTEnS9OnTNXjwYJWWlkqSJk+erEWLFuncc89VQUGB3nvvPd11112aPHlyNPAAAAAAQHuKO+hMnTpVBw4c0Pz581VdXa3Ro0dr9erV0QUKdu3aFTOCc+edd8rhcOjOO+9UVVWV+vXrp8mTJ+vee+9tv3cBAADQRqYZ6Ynj8bCwAGAnDisF5o/V1tYqJydHoVBI2dnZiS4HAADYhGlKXm9jLxyWiQaSX2uzQYevugYAAJCsgsHGkON0SuXlia4IQHsh6AAAgLTl8TSGnHBYcrsTXRGA9hL3d3QAAADswjAi09XKyyMhh2lrgH0QdAAAQFozDAIOYEdMXQMAAABgOwQdAAAAALZD0AEAAABgOwQdAAAAALZD0AEAALZgmpLPF9kCAEEHAACkPNOUvF7J749sCTsACDoAACDlBYONTT+dzkhfHADpjaADAABSnsfTGHLC4UjzTwDpjYahAAAg5RmGFAhERnLcbhqAAiDoAAAAmzAMAg6ARkxdAwAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAScM0JZ+Php8ATh1BBwAAJAXTlLxeye+PbAk7AE4FQQcAACSFYLCx4afTGemJAwBtRdABAABJweNpDDnhcKTxJwC0FQ1DAQBAUjAMKRCIjOS43TT/BHBqCDoAACBpGAYBB0D7YOoaAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAABod6Yp+Xw0/QSQOAQdAADQrkxT8nolvz+yJewASASCDgAAaFfBYGPTT6cz0hcHADobQQcAALQrj6cx5ITDkeafANDZaBgKAADalWFIgUBkJMftpgEogMQg6AAAgHZnGAQcAInF1DUAAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AANAi05R8Ppp+Akg9BB0AANAs05S8Xsnvj2wJOwBSCUEHAAA0KxhsbPrpdEb64gBAqiDoAACAZnk8jSEnHI40/wSAVEHDUAAA0CzDkAKByEiO200DUACphaADAABaZBgEHACpialrAAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6AADYnGlKPh8NPwGkF4IOAAA2ZpqS1yv5/ZEtYQdAuiDoAABgY8FgY8NPpzPSEwcA0gFBBwAAG/N4GkNOOBxp/AkA6YCGoQAA2JhhSIFAZCTH7ab5J4D0QdABAMDmDIOAAyD9MHUNAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAIAUYZqSz0fTTwBoDYIOAAApwDQlr1fy+yNbwg4AnFibgs6SJUuUn5+vrKwsFRQUaP369S0e63a75XA4mjwuu+yyNhcNAEC6CQYbm346nZG+OACAlsUddFasWKHi4mKVlJRo48aNGjVqlIqKirR///5mj1+5cqX27dsXfWzevFlOp1Pf/va3T7l4AADShcfTGHLC4UjzTwBAyxyWZVnxnFBQUKDzzz9fDz30kCSpoaFBeXl5uuWWWzR37tyTnr948WLNnz9f+/bt0+c+97lWXbO2tlY5OTkKhULKzs6Op1wAAGzDNCMjOW43DUABpK/WZoMu8bxofX29NmzYoHnz5kX3ZWRkqLCwUBUVFa16jaVLl+qqq646Ycipq6tTXV1d9M+1tbXxlAkAgC0ZBgEHAForrqlrBw8eVDgcVm5ubsz+3NxcVVdXn/T89evXa/PmzbrhhhtOeFxpaalycnKij7y8vHjKBAAAAJDmOnXVtaVLl2rkyJEaN27cCY+bN2+eQqFQ9LF79+5OqhAAAACAHcQ1da1v375yOp2qqamJ2V9TU6MBAwac8NzDhw9r+fLluvvuu096HZfLJZfLFU9pAAAAABAV14hOZmamxowZo7Kysui+hoYGlZWVafz48Sc899lnn1VdXZ2uvfbatlUKAAAAAK0U99S14uJiPf7443rqqae0ZcsW3XTTTTp8+LBmzpwpSZo+fXrMYgXHLV26VFOmTFGfPn1OvWoAAFKYaUo+H00/AaAjxTV1TZKmTp2qAwcOaP78+aqurtbo0aO1evXq6AIFu3btUkZGbH6qrKzUunXr9PLLL7dP1QAApCjTlLzeSD+cxYulQICV1ACgI8TdRycR6KMDALALn0/y+xubf956q7RoUaKrAoDU0dps0KmrrgEAkO48nsaQEw5Hmn8CANpf3FPXAABA2xlGZLpaeXkk5DBtDQA6BkEHAIBOZhgEHADoaExdAwAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQCgDUwz0hPHNBNdCQCgOQQdAADiZJqS1xtp/On1EnYAIBkRdAAAiFMw2Njw0+mM9MQBACQXgg4AAHHyeBpDTjgcafwJAEguNAwFACBOhiEFApGRHLeb5p8AkIwIOgAAtIFhEHAAIJkxdQ0AAACA7RB0AAAAANgOQQcAAACA7RB0AAAAANgOQQcAkNZMU/L5aPoJAHZD0AEApC3TlLxeye+PbAk7AGAfBB0AQNoKBhubfjqdkb44AAB7IOgAANKWx9MYcsLhSPNPAIA90DAUAJC2DEMKBCIjOW43DUABwE4IOgCAtGYYBBwAsCOmrgEAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAUp5pSj4fDT8BAI0IOgCAlGaaktcr+f2RLWEHACARdAAAKS4YbGz46XRGeuIAAEDQAQCkNI+nMeSEw5HGnwAA0DAUAJDSDEMKBCIjOW43zT8BABEEHQBAyjMMAg4AIBZT1wAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAScM0JZ+Ppp8AgFNH0AEAJAXTlLxeye+PbAk7AIBTQdABACSFYLCx6afTGemLAwBAWxF0AABJweNpDDnhcKT5JwAAbUXDUABAUjAMKRCIjOS43TQABQCcGoIOACBpGAYBBwDQPpi6BgAAAMB2CDoAAAAAbIegAwAAAMB2CDoAAAAAbIegAwBod6Yp+Xw0/QQAJA5BBwDQrkxT8nolvz+yJewAABKBoAMAaFfBYGPTT6cz0hcHAIDORtABALQrj6cx5ITDkeafAAB0NhqGAgDalWFIgUBkJMftpgEoACAxCDoAgHZnGAQcAEBiMXUNAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHANAs05R8Php+AgBSE0EHANCEaUper+T3R7aEHQBAqiHoAACaCAYbG346nZGeOAAApBKCDgCgCY+nMeSEw5HGnwAApJI2BZ0lS5YoPz9fWVlZKigo0Pr16094/EcffaTZs2dr4MCBcrlcOvPMM/XSSy+1qWAAQMczDCkQkG69NbKl+ScAINV0ifeEFStWqLi4WI8++qgKCgq0ePFiFRUVqbKyUv37929yfH19vS6++GL1799fzz33nAYPHqydO3eqV69e7VE/AKCDGAYBBwCQuhyWZVnxnFBQUKDzzz9fDz30kCSpoaFBeXl5uuWWWzR37twmxz/66KP62c9+pq1bt6pr166tukZdXZ3q6uqif66trVVeXp5CoZCys7PjKRcAAACAjdTW1ionJ+ek2SCuqWv19fXasGGDCgsLG18gI0OFhYWqqKho9hzTNDV+/HjNnj1bubm5+vKXv6z77rtP4XC4xeuUlpYqJycn+sjLy4unTAAAAABpLq6gc/DgQYXDYeXm5sbsz83NVXV1dbPnbNu2Tc8995zC4bBeeukl3XXXXXrwwQf1k5/8pMXrzJs3T6FQKPrYvXt3PGUCAAAASHNxf0cnXg0NDerfv79+9atfyel0asyYMaqqqtLPfvYzlZSUNHuOy+WSy+Xq6NIAAAAA2FRcQadv375yOp2qqamJ2V9TU6MBAwY0e87AgQPVtWtXOZ3O6L4RI0aourpa9fX1yszMbEPZAIDWMs1IXxyPh8UFAADpI66pa5mZmRozZozKysqi+xoaGlRWVqbx48c3e87EiRP13nvvqaGhIbrv3Xff1cCBAwk5ANDBTFPyeiW/P7I1zURXBABA54i7j05xcbEef/xxPfXUU9qyZYtuuukmHT58WDNnzpQkTZ8+XfPmzYsef9NNN+mDDz7QD3/4Q7377rtatWqV7rvvPs2ePbv93gUAoFnBYGPTT6dTKi9PdEUAAHSOuL+jM3XqVB04cEDz589XdXW1Ro8erdWrV0cXKNi1a5cyMhrzU15env70pz/J5/PpnHPO0eDBg/XDH/5Qt99+e/u9CwBAszweafHixrDjdie6IgAAOkfcfXQSobVrZQMAmjLNyEiO2813dAAAqa+12aDDV10DACSWYRBwAADpJ+7v6AAAAABAsiPoAAAAALAdgg4AAAAA2yHoAAAAALAdgg4ApAjTlHw+mn4CANAaBB0ASAGmKXm9kt8f2RJ2AAA4MYIOAKSAYLCx6afTGemLAwAAWkbQAYAU4PE0hpxwONL8EwAAtIyGoQCQAgxDCgQiIzluNw1AAQA4GYIOAKQIwyDgAADQWkxdAwAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAYBOZJqSz0fDTwAAOhpBBwA6iWlKXq/k90e2hB0AADoOQQcAOkkw2Njw0+mM9MQBAAAdg6ADAJ3E42kMOeFwpPEnAADoGDQMBYBOYhhSIBAZyXG7af4JAEBHIugAQCcyDAIOAACdgalrAAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6ANAGpin5fDT9BAAgWRF0ACBOpil5vZLfH9kSdgAASD4EHQCIUzDY2PTT6Yz0xQEAAMmFoAMAcfJ4GkNOOBxp/gkAAJILDUMBIE6GIQUCkZEct5sGoAAAJCOCDgC0gWEQcAAASGZMXQMAAABgOwQdAAAAALZD0AEAAABgOwQdAAAAALZD0AGQtkxT8vlo+AkAgB0RdACkJdOUvF7J749sCTsAANgLQQdAWgoGGxt+Op2RnjgAAMA+CDoA0pLH0xhywuFI408AAGAfNAwFkJYMQwoEIiM5bjfNPwEAsBuCDoC0ZRgEHAAA7IqpawAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgBSnmlKPh9NPwEAQCOCDoCUZpqS1yv5/ZEtYQcAAEgEHQApLhhsbPrpdEb64gAAABB0AKQ0j6cx5ITDkeafAAAANAwFkNIMQwoEIiM5bjcNQAEAQARBB0DKMwwCDgAAiMXUNQAAAAC2Q9ABAAAAYDsEHQAAAAC2Q9ABAAAAYDsEHQBJwzQln4+mnwAA4NQRdAAkBdOUvF7J749sCTsAAOBUEHQAJIVgsLHpp9MZ6YsDAADQVgQdAEnB42kMOeFwpPknAABAW9EwFEBSMAwpEIiM5LjdNAAFAACnpk0jOkuWLFF+fr6ysrJUUFCg9evXt3jssmXL5HA4Yh5ZWVltLhiAfRmGtGgRIQcAAJy6uIPOihUrVFxcrJKSEm3cuFGjRo1SUVGR9u/f3+I52dnZ2rdvX/Sxc+fOUyoaAAAAAE4k7qCzaNEi3XjjjZo5c6bOPvtsPfroo+revbueeOKJFs9xOBwaMGBA9JGbm3tKRQMAAADAicQVdOrr67VhwwYVFhY2vkBGhgoLC1VRUdHieR9//LGGDh2qvLw8eb1evf322ye8Tl1dnWpra2MeAAAAANBacQWdgwcPKhwONxmRyc3NVXV1dbPnDB8+XE888YQCgYCeeeYZNTQ0aMKECdqzZ0+L1yktLVVOTk70kZeXF0+ZAAAAANJchy8vPX78eE2fPl2jR4/WpEmTtHLlSvXr10+PPfZYi+fMmzdPoVAo+ti9e3dHlwmgnZim5PPR8BMAACRWXMtL9+3bV06nUzU1NTH7a2pqNGDAgFa9RteuXXXuuefqvffea/EYl8sll8sVT2kAkoBpSl5vpBfO4sWR5aJZQQ0AACRCXCM6mZmZGjNmjMrKyqL7GhoaVFZWpvHjx7fqNcLhsN566y0NHDgwvkoBJL1gsLHhp9MZ6YkDAACQCHFPXSsuLtbjjz+up556Slu2bNFNN92kw4cPa+bMmZKk6dOna968edHj7777br388svatm2bNm7cqGuvvVY7d+7UDTfc0H7vAkBS8HgaQ044HGn8CQAAkAhxTV2TpKlTp+rAgQOaP3++qqurNXr0aK1evTq6QMGuXbuUkdGYnz788EPdeOONqq6u1uc//3mNGTNGr7/+us4+++z2excAkoJhRKarlZdHQg7T1gAAQKI4LMuyEl3EydTW1ionJ0ehUEjZ2dmJLgcAAABAgrQ2G3T4qmsAAAAA0NkIOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADoFmmKfl8kS0AAECqIegAaMI0Ja9X8vsjW8IOAABINQQdAE0Eg41NP53OSF8cAACAVELQAdCEx9MYcsLhSPNPAACAVNIl0QUASD6GIQUCkZEctzvyZwAAgFRC0AHQLMMg4AAAgNTF1DUAAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB3AxkxT8vlo+AkAANIPQQewKdOUvF7J749sCTsAACCdEHQAmwoGGxt+Op2RnjgAAADpgqAD2JTH0xhywuFI408AAIB0QcNQwKYMQwoEIiM5bjfNPwEAQHoh6AA2ZhgEHAAAkJ6YugYAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAOkANOUfD6afgIAALQWQQdIcqYpeb2S3x/ZEnYAAABOjqADJLlgsLHpp9MZ6YsDAACAEyPoAEnO42kMOeFwpPknAAAAToyGoUCSMwwpEIiM5LjdNAAFAABoDYIOkAIMg4ADAAAQD6auAQAAALAdgg4AAAAA2yHoAAAAALAdgg4AAAAA2yHoAJ3INCWfj6afAAAAHY2gA3QS05S8Xsnvj2wJOwAAAB2HoAN0kmCwsemn0xnpiwMAAICOQdABOonH0xhywuFI808AAAB0DBqGAp3EMKRAIDKS43bTABQAAKAjEXSATmQYBBwAAIDOwNQ1AAAAALZD0AEAAABgOwQdAAAAALZD0AEAAABgOwQdIE6mKfl8NPwEAABIZgQdIA6mKXm9kt8f2RJ2AAAAkhNBB4hDMNjY8NPpjPTEAQAAQPIh6ABx8HgaQ044HGn8CQAAgORDw1AgDoYhBQKRkRy3m+afAAAAyYqgA8TJMAg4AAAAyY6pawAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOkhbpin5fDT9BAAAsCOCDtKSaUper+T3R7aEHQAAAHsh6CAtBYONTT+dzkhfHAAAANgHQQdpyeNpDDnhcKT5JwAAAOyDhqFIS4YhBQKRkRy3mwagAAAAdkPQQdoyDAIOAACAXTF1DQAAAIDttCnoLFmyRPn5+crKylJBQYHWr1/fqvOWL18uh8OhKVOmtOWyAAAAANAqcQedFStWqLi4WCUlJdq4caNGjRqloqIi7d+//4Tn7dixQ7fddpsuvPDCNhcLAAAAAK0Rd9BZtGiRbrzxRs2cOVNnn322Hn30UXXv3l1PPPFEi+eEw2F95zvf0cKFC3X66aef9Bp1dXWqra2NeQAAAABAa8UVdOrr67VhwwYVFhY2vkBGhgoLC1VRUdHieXfffbf69++v66+/vlXXKS0tVU5OTvSRl5cXT5lIM6Yp+Xw0/QQAAECjuILOwYMHFQ6HlZubG7M/NzdX1dXVzZ6zbt06LV26VI8//nirrzNv3jyFQqHoY/fu3fGUiTRimpLXK/n9kS1hBwAAAFIHr7p26NAhTZs2TY8//rj69u3b6vNcLpeys7NjHkBzgsHGpp9OZ6QvDgAAABBXH52+ffvK6XSqpqYmZn9NTY0GDBjQ5Pj3339fO3bs0OTJk6P7GhoaIhfu0kWVlZU644wz2lI3IEnyeKTFixvDjtud6IoAAACQDOIa0cnMzNSYMWNUVlYW3dfQ0KCysjKNHz++yfFnnXWW3nrrLW3atCn6MAxDHo9HmzZt4rs3OGWGIQUC0q23RrY0AAUAAIAU54iOJBUXF2vGjBkaO3asxo0bp8WLF+vw4cOaOXOmJGn69OkaPHiwSktLlZWVpS9/+csx5/fq1UuSmuwH2sowCDgAAACIFXfQmTp1qg4cOKD58+erurpao0eP1urVq6MLFOzatUsZGR361R8AAAAAOCGHZVlWoos4mdraWuXk5CgUCrEwAQAAAJDGWpsNGHoBAAAAYDsEHQAAAAC2Q9BBUjBNyeej4ScAAADaB0EHCWeaktcr+f2RLWEHAAAAp4qgg4QLBhsbfjqdUnl5oisCAABAqiPoIOE8nsaQEw5LbneiKwIAAECqi7uPDtDeDEMKBCIjOW43zT8BAABw6gg6SAqGQcABAABA+2HqGgAAAADbIegAAAAAsB2CDgAAAADbIegAAAAAsB2CDtqVaUo+H00/AQAAkFgEHbQb05S8Xsnvj2wJOwAAAEgUgg7aTTDY2PTT6Yz0xQEAAAASgaCDduPxNIaccDjS/BMAAABIBBqGot0YhhQIREZy3G4agAIAACBxCDpoV4ZBwAEAAEDiMXUNAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHTZim5PPR8BMAAACpi6CDGKYpeb2S3x/ZEnYAAACQigg6iBEMNjb8dDojPXEAAACAVEPQQQyPpzHkhMORxp8AAABAqqFhKGIYhhQIREZy3G6afwIAACA1EXTQhGEQcAAAAJDamLoGAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAABsh6BjY6Yp+Xw0/QQAAED6IejYlGlKXq/k90e2hB0AAACkE4KOTQWDjU0/nc5IXxwAAAAgXRB0bMrjaQw54XCk+ScAAACQLmgYalOGIQUCkZEct5sGoAAAAEgvBB0bMwwCDgAAANITU9cAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHRSgGlKPh9NPwEAAIDWIugkOdOUvF7J749sCTsAAADAyRF0klww2Nj00+mM9MUBAAAAcGIEnSTn8TSGnHA40vwTAAAAwInRMDTJGYYUCERGctxuGoACAAAArUHQSQGGQcABAAAA4sHUNQAAAAC2Q9ABAAAAYDsEHQAAAAC2Q9ABAAAAYDsEnU5impLPR8NPAAAAoDMQdDqBaUper+T3R7aEHQAAAKBjEXQ6QTDY2PDT6Yz0xAEAAADQcQg6ncDjaQw54XCk8ScAAACAjkPD0E5gGFIgEBnJcbtp/gkAAAB0NIJOJzEMAg4AAADQWZi6BgAAAMB2CDoAAAAAbKdNQWfJkiXKz89XVlaWCgoKtH79+haPXblypcaOHatevXrpc5/7nEaPHq2nn366zQUDAAAAwMnEHXRWrFih4uJilZSUaOPGjRo1apSKioq0f//+Zo/v3bu37rjjDlVUVOif//ynZs6cqZkzZ+pPf/rTKRcPAAAAAM1xWJZlxXNCQUGBzj//fD300EOSpIaGBuXl5emWW27R3LlzW/Ua5513ni677DLdc889rTq+trZWOTk5CoVCys7Ojqfcdmeakb44Hg+LCwAAAACdrbXZIK4Rnfr6em3YsEGFhYWNL5CRocLCQlVUVJz0fMuyVFZWpsrKSn31q19t8bi6ujrV1tbGPJKBaUper+T3R7ammeiKAAAAADQnrqBz8OBBhcNh5ebmxuzPzc1VdXV1i+eFQiH16NFDmZmZuuyyy+T3+3XxxRe3eHxpaalycnKij7y8vHjK7DDBYGPTT6cz0hcHAAAAQPLplFXXevbsqU2bNulvf/ub7r33XhUXF6v8BClh3rx5CoVC0cfu3bs7o8yT8ngaQ044HGn+CQAAACD5xNUwtG/fvnI6naqpqYnZX1NTowEDBrR4XkZGhr7whS9IkkaPHq0tW7aotLRU7haSgsvlksvliqe0TmEYUiAQGclxu/mODgAAAJCs4hrRyczM1JgxY1RWVhbd19DQoLKyMo0fP77Vr9PQ0KC6urp4Lp00DENatIiQAwAAACSzuEZ0JKm4uFgzZszQ2LFjNW7cOC1evFiHDx/WzJkzJUnTp0/X4MGDVVpaKinyfZuxY8fqjDPOUF1dnV566SU9/fTTeuSRR9r3nQAAAADA/xN30Jk6daoOHDig+fPnq7q6WqNHj9bq1aujCxTs2rVLGRmNA0WHDx/WD37wA+3Zs0fdunXTWWedpWeeeUZTp05tv3cBAAAAAJ8Rdx+dREimPjoAAAAAEqdD+ugAAAAAQCog6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwHYIOAAAAANsh6AAAAACwnS6JLqA1LMuSJNXW1ia4EgAAAACJdDwTHM8ILUmJoHPo0CFJUl5eXoIrAQAAAJAMDh06pJycnBafd1gni0JJoKGhQXv37lXPnj3lcDgSWkttba3y8vK0e/duZWdnJ7QWpB7uH5wK7h+0FfcOTgX3D05FR9w/lmXp0KFDGjRokDIyWv4mTkqM6GRkZGjIkCGJLiNGdnY2P+xoM+4fnAruH7QV9w5OBfcPTkV73z8nGsk5jsUIAAAAANgOQQcAAACA7RB04uRyuVRSUiKXy5XoUpCCuH9wKrh/0FbcOzgV3D84FYm8f1JiMQIAAAAAiAcjOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAABsh6DTjCVLlig/P19ZWVkqKCjQ+vXrT3j8s88+q7POOktZWVkaOXKkXnrppU6qFMkonvvn8ccf14UXXqjPf/7z+vznP6/CwsKT3m+wr3j/7Tlu+fLlcjgcmjJlSscWiKQW7/3z0Ucfafbs2Ro4cKBcLpfOPPNM/v8rjcV7/yxevFjDhw9Xt27dlJeXJ5/PpyNHjnRStUgWr776qiZPnqxBgwbJ4XDoxRdfPOk55eXlOu+88+RyufSFL3xBy5Yt67D6CDr/YcWKFSouLlZJSYk2btyoUaNGqaioSPv372/2+Ndff11XX321rr/+er355puaMmWKpkyZos2bN3dy5UgG8d4/5eXluvrqqxUMBlVRUaG8vDxdcsklqqqq6uTKkWjx3jvH7dixQ7fddpsuvPDCTqoUySje+6e+vl4XX3yxduzYoeeee06VlZV6/PHHNXjw4E6uHMkg3vvnd7/7nebOnauSkhJt2bJFS5cu1YoVK/TjH/+4kytHoh0+fFijRo3SkiVLWnX89u3bddlll8nj8WjTpk367//+b91www3605/+1DEFWogxbtw4a/bs2dE/h8Nha9CgQVZpaWmzx1955ZXWZZddFrOvoKDA+v73v9+hdSI5xXv//Kdjx45ZPXv2tJ566qmOKhFJqi33zrFjx6wJEyZYv/71r60ZM2ZYXq+3EypFMor3/nnkkUes008/3aqvr++sEpHE4r1/Zs+ebX3ta1+L2VdcXGxNnDixQ+tEcpNkvfDCCyc8Zs6cOdaXvvSlmH1Tp061ioqKOqQmRnQ+o76+Xhs2bFBhYWF0X0ZGhgoLC1VRUdHsORUVFTHHS1JRUVGLx8O+2nL//KdPPvlER48eVe/evTuqTCShtt47d999t/r376/rr7++M8pEkmrL/WOapsaPH6/Zs2crNzdXX/7yl3XfffcpHA53VtlIEm25fyZMmKANGzZEp7dt27ZNL730kr7xjW90Ss1IXZ39e3OXDnnVFHXw4EGFw2Hl5ubG7M/NzdXWrVubPae6urrZ46urqzusTiSnttw//+n222/XoEGDmvwjAHtry72zbt06LV26VJs2beqECpHM2nL/bNu2Ta+88oq+853v6KWXXtJ7772nH/zgBzp69KhKSko6o2wkibbcP9dcc40OHjyoCy64QJZl6dixY5o1axZT13BSLf3eXFtbq08//VTdunVr1+sxogMkifvvv1/Lly/XCy+8oKysrESXgyR26NAhTZs2TY8//rj69u2b6HKQghoaGtS/f3/96le/0pgxYzR16lTdcccdevTRRxNdGlJAeXm57rvvPj388MPauHGjVq5cqVWrVumee+5JdGlADEZ0PqNv375yOp2qqamJ2V9TU6MBAwY0e86AAQPiOh721Zb757gHHnhA999/v9asWaNzzjmnI8tEEor33nn//fe1Y8cOTZ48ObqvoaFBktSlSxdVVlbqjDPO6NiikTTa8m/PwIED1bVrVzmdzui+ESNGqLq6WvX19crMzOzQmpE82nL/3HXXXZo2bZpuuOEGSdLIkSN1+PBhfe9739Mdd9yhjAz+Ozqa19LvzdnZ2e0+miMxohMjMzNTY8aMUVlZWXRfQ0ODysrKNH78+GbPGT9+fMzxkvTnP/+5xeNhX225fyTpf//3f3XPPfdo9erVGjt2bGeUiiQT771z1lln6a233tKmTZuiD8MwoqvY5OXldWb5SLC2/NszceJEvffee9GALEnvvvuuBg4cSMhJM225fz755JMmYeZ4aI58Jx1oXqf/3twhSxyksOXLl1sul8tatmyZ9c4771jf+973rF69elnV1dWWZVnWtGnTrLlz50aPf+2116wuXbpYDzzwgLVlyxarpKTE6tq1q/XWW28l6i0ggeK9f+6//34rMzPTeu6556x9+/ZFH4cOHUrUW0CCxHvv/CdWXUtv8d4/u3btsnr27GndfPPNVmVlpfXHP/7R6t+/v/WTn/wkUW8BCRTv/VNSUmL17NnT+v3vf29t27bNevnll60zzjjDuvLKKxP1FpAghw4dst58803rzTfftCRZixYtst58801r586dlmVZ1ty5c61p06ZFj9+2bZvVvXt363/+53+sLVu2WEuWLLGcTqe1evXqDqmPoNMMv99vnXbaaVZmZqY1btw464033og+N2nSJGvGjBkxx//hD3+wzjzzTCszM9P60pe+ZK1ataqTK0Yyief+GTp0qCWpyaOkpKTzC0fCxftvz2cRdBDv/fP6669bBQUFlsvlsk4//XTr3nvvtY4dO9bJVSNZxHP/HD161FqwYIF1xhlnWFlZWVZeXp71gx/8wPrwww87v3AkVDAYbPb3mOP3y4wZM6xJkyY1OWf06NFWZmamdfrpp1tPPvlkh9XnsCzGGAEAAADYC9/RAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7/z/sc7TdBt/4dAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build Model\n",
        "Our first PyTorch model !\n",
        "\n",
        "This is very exciting..let's do it !!!\n",
        "\n",
        "What our model does :\n",
        "* Start with random values (weights and bias)\n",
        "* Look at training data and adjust the random values to better represent(or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
        "\n",
        "How does it do so ?\n",
        "\n",
        "It uses 2 algorithms:\n",
        "1. Gradient Desecnt\n",
        "2. Backpropagation"
      ],
      "metadata": {
        "id": "GsJQZid6inWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create linear regression model class\n",
        "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is built from nn.Module (base class for a neural network modules)\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                            requires_grad=True,\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                             requires_grad=True,\n",
        "                             dtype=torch.float))\n",
        "\n",
        "  # Forward method to define the computation in the model\n",
        "  def forward(self,x:torch.Tensor) -> torch.Tensor: #<- \"x\" is the input data\n",
        "    return self.weights * x + self.bias # this is the linear regression formula\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-42uURNiNZH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch model building essentials\n",
        "* `torch.nn` - contains all of the buildings for computational graphs ( a neural network can be considered a computational graph)\n",
        "* `torch.nn.Parameter` - what paramters should our model try and learn, often a PyTorch layer from `torch.nn` will set these for us\n",
        "* `torch.nn.Module` - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
        "* `torch.optim` - this where the optimizers in PyTorch live, they will help with gradient descent\n",
        "* `def forward()` - All `nn.Module` subclasses require you to overwrite `forward()`, this method defines what happens in forward computation"
      ],
      "metadata": {
        "id": "0VwpUEm2miG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the contents of our PyTorch model\n",
        "Now we've created a model, let's see what's inside ...\n",
        "\n",
        "So we can check model parameters or what's inside our model using `.paramters()`"
      ],
      "metadata": {
        "id": "sdibpS8qoktD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random seed\n",
        "torch.manual_seed(seed=42)\n",
        "\n",
        "# Create an instance of the model (this is a subclass of nn.Module)\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "# Check out the paramters\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkXcIfE7lT1f",
        "outputId": "002d5c36-b1ac-471b-c70f-f77bde552be6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvgz9kUhpL16",
        "outputId": "945cb856-f470-4eaa-d4af-adf430ea8845"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions using `torch.inference_mode()`\n",
        "To check our model's predictive power, let's see how well it predicts `y_test` based on `x_test`.\n",
        "\n",
        "When we pass data through our model, it's going to run it through the `forward()` method."
      ],
      "metadata": {
        "id": "gj1a2IRzera7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions with model\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "id": "L4IDGt0ppuOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7464203d-25df-4853-e0bb-867d3df8e551"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_2 = model_0(X_test).detach().numpy()\n",
        "y_preds_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DPdNXNIhB1A",
        "outputId": "e6d2a505-fad9-4c79-e985-c194fb2cda78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3981617 ],\n",
              "       [0.40489548],\n",
              "       [0.41162932],\n",
              "       [0.41836315],\n",
              "       [0.42509693],\n",
              "       [0.43183076],\n",
              "       [0.43856454],\n",
              "       [0.44529837],\n",
              "       [0.45203215],\n",
              "       [0.45876598]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FgazfYCAfd2b",
        "outputId": "9ff87148-b040-4da5-893b-78043cbf4687"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "The whole idea of training is for a model to move from some unknown paramters (these may be random) to some known parameters.\n",
        "\n",
        "Or in other words from a poor representation of the data to a better representation of the data.\n",
        "\n",
        "One way to measure how poor or how wrong your model's predictions are is to use a loss function.\n",
        "\n",
        "* Note: Loss functio may also be called criterion in different areas. For our case, we're going to refer it as a loss function.\n",
        "\n",
        "Things we need to train:\n",
        "* **Loss Function**: A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.\n",
        "* **Optimizer**: Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function.\n",
        "\n",
        "    Inside the optimizer you'll have to set two parameters:\n",
        "    * `params` - the model parameters you'll like to potimize, for example `params=model_0.parameters()`\n",
        "    * `lr` (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step ( a small `lr` results in small change, a large `lr` results in large changes)\n",
        "\n",
        "And specifically for PyTorch, we need:\n",
        "* A training loop\n",
        "* A testing loop"
      ],
      "metadata": {
        "id": "kbBgaVxlhliZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# setting up an optimizer (stochastic gradient descent)\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)  # lr -> learning rate = possibly the most import hyperparamter that we can set"
      ],
      "metadata": {
        "id": "5GUYITFdf6VP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q**: WHich loss function and optimizer should I use ?\n",
        "\n",
        "**A**: This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.\n",
        "\n",
        "For example, for a regression problem (likeours, a loss function of `nn.L1Loss()` and an optimizer like `torch.optim.SGD()` will suffice.\n",
        "\n",
        "But for a classification problem like classifying whether a photo is of a dog or a car, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss)"
      ],
      "metadata": {
        "id": "Y6B2bZWkmkKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a training loop (and a testing loop) in PyTorch\n",
        "A couple of things we need in a training loop:\n",
        "0. Loop through the data\n",
        "1. Forward pass (this involves data moving through our model's `forward()` functions) - also called forward propagation\n",
        "2. Calculate the loss (compare the forward pass predictions to ground truth labels)\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters ofnour model with respect to the loss (**backpropagation**)\n",
        "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)"
      ],
      "metadata": {
        "id": "-CPHKaPOnhZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoYpOI-spVVu",
        "outputId": "91c6fe70-2d93-4589-ebb0-e828266ec197"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up a seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# an epoch is one loop through the data...(this is a hyperparameter because we've set them)\n",
        "epochs = 200\n",
        "\n",
        "# Track different values\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "# Training\n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs):\n",
        "  # set the model to training mode\n",
        "  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  # print(f\"Loss:{loss}\")\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer (perform gradient descent)\n",
        "  optimizer.step()  # by default how the optimizer changes wil; accumulate through the loop so... we have to zero them above in step 3 for the next iteration of the loop\n",
        "\n",
        "  # Testing\n",
        "  model_0.eval()  # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
        "  with torch.inference_mode():  # turns off gradient tracking & a couple more things behind the scenes\n",
        "  # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code\n",
        "    # 1. Do the forward pass\n",
        "    test_pred = model_0(X_test)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "  if epoch % 10 == 0:\n",
        "    epoch_count.append(epoch)\n",
        "    loss_values.append(loss)\n",
        "    test_loss_values.append(test_loss)\n",
        "    pprint(f\"Epoch:{epoch} | Loss:{loss} | Test Loss:{test_loss}\")\n",
        "    # print out the model weights and biases\n",
        "    pprint(model_0.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRlcMeR9kOME",
        "outputId": "ae242c66-a9be-421a-cf99-35bc7d60579b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Epoch:0 | Loss:0.31288138031959534 | Test Loss:0.35982614755630493'\n",
            "OrderedDict({'weights': tensor([0.3757]), 'bias': tensor([0.2288])})\n",
            "'Epoch:10 | Loss:0.025432366877794266 | Test Loss:0.05427704378962517'\n",
            "OrderedDict({'weights': tensor([0.5898]), 'bias': tensor([0.3438])})\n",
            "'Epoch:20 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:30 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:40 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:50 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:60 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:70 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:80 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:90 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:100 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:110 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:120 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:130 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:140 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:150 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:160 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:170 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:180 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n",
            "'Epoch:190 | Loss:0.039773717522621155 | Test Loss:0.11934101581573486'\n",
            "OrderedDict({'weights': tensor([0.6122]), 'bias': tensor([0.2588])})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss curves\n",
        "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train Loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test Loss\")\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "ENjGKvri7FGk",
        "outputId": "1131c68d-fd77-4b04-d392-7b394edc7707"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-406978220.py:2: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train Loss\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUKCzwZ_p5mh",
        "outputId": "a221f9c4-481c-4a8d-bc9e-8af7585ae461"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6512])), ('bias', tensor([0.3588]))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds_new = model_0(X_test)\n",
        "\n",
        "y_preds_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwge_2Ut1q6-",
        "outputId": "4ab6a9e2-386d-43ab-f621-819494ca96c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8798],\n",
              "        [0.8928],\n",
              "        [0.9058],\n",
              "        [0.9188],\n",
              "        [0.9319],\n",
              "        [0.9449],\n",
              "        [0.9579],\n",
              "        [0.9709],\n",
              "        [0.9840],\n",
              "        [0.9970]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08HyWr096Kxf",
        "outputId": "ea5097ff-b762-4806-f1cc-eb3be0cd9d5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "AL12dwEV2MnA",
        "outputId": "16377240-f628-479f-a34b-27505b1fdc19"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving a model in PyTorch\n",
        "There are 3 main methods you should know about saving and loading models in PyTorch.\n",
        "1. `torch.save()` - allows you to save a PyTorch object in Python's pickle format\n",
        "2. `torch.load()` - allows you to load a saved PyTorch object\n",
        "3. `torch.nn.Module.load_state_dict()` - this allows to load a model's saved state dictionary"
      ],
      "metadata": {
        "id": "PdlXQUdz9hrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving our PyTorch model\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create a model directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to:{MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEphjy4l21LS",
        "outputId": "40eb9ceb-415e-406d-faae-4cc44c7860a4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to:models/01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWvq4m0y_LRQ",
        "outputId": "722d83d5-ee6c-43e9-81f6-33b5da2377bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -1 models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5KUEsIZ_oie",
        "outputId": "8f1fff13-9591-470e-8ec8-6623b76beac8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a PyTorch model\n",
        "Since we saved our model's `state_dict()` rather the entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that"
      ],
      "metadata": {
        "id": "9O_h0sc6ApVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODXqKQsQ_wtH",
        "outputId": "7262caf0-0bfd-48c5-d835-f93e8bfcde35"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6512])), ('bias', tensor([0.3588]))])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load in a saved state_dict we have to instantiate a new instance of our model class\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the saved state_dict of model_0 (this will update the new instance with uploaded parameters)\n",
        "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uBObSt0BDft",
        "outputId": "e7e7f544-08d9-4f94-ca8b-eaaafa2d1a38"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jBsP1kmBeN-",
        "outputId": "8e0807df-12b8-45d6-bef5-b86b43ad6c8a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6512])), ('bias', tensor([0.3588]))])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions with our loaded model\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_preds = loaded_model_0(X_test)\n",
        "\n",
        "loaded_model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BTNtVyaBgTG",
        "outputId": "20dda620-cca8-45b4-cbc5-7740b0e494b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8798],\n",
              "        [0.8928],\n",
              "        [0.9058],\n",
              "        [0.9188],\n",
              "        [0.9319],\n",
              "        [0.9449],\n",
              "        [0.9579],\n",
              "        [0.9709],\n",
              "        [0.9840],\n",
              "        [0.9970]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare loaded model preds woth original model preds\n",
        "og_model_preds = model_0(X_test)\n",
        "print(og_model_preds == loaded_model_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pr5Mlr8B9Fe",
        "outputId": "b098a3b3-47f7-4ed1-e9bb-d632a203548b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Putting it all together\n",
        "Let's go back through the steps above and see it all in one place"
      ],
      "metadata": {
        "id": "8vqlAtjrCqjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check pyTorch version\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lma9vog1CLFw",
        "outputId": "9cf21418-d9e7-4ac7-c1c4-127b2f6135e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create device-agnostic code.\n",
        "\n",
        "This means if we've got access to a GPU, our code will use it (for potentially faster computing).\n",
        "\n",
        "If no GPU is available, the code will default to using CPU."
      ],
      "metadata": {
        "id": "pDQrxG5-DRs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() == True else \"cpu\"\n",
        "print(f\"Using Device:{device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoiD-_VfDQpK",
        "outputId": "6eba6f8b-9899-475c-b42f-897ce43b1a8e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device:cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMo83Mc6DsHF",
        "outputId": "84bc9763-3ffa-4044-f99f-4314bf88d273"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 17 19:30:13 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Data"
      ],
      "metadata": {
        "id": "eUAhWQ5DC0AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating some synthetic data using the regression formula (y = m*x + c)\n",
        "weight = 0.29837\n",
        "bias = 12.210\n",
        "\n",
        "# Create range values\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "# Create X and y (features and labels)\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up\n",
        "y = weight * X + bias\n",
        "X[:10], y[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxMh4hlsD_ia",
        "outputId": "ebc362bd-a9a4-4539-ac16-f87863b9f49d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[12.2100],\n",
              "         [12.2160],\n",
              "         [12.2219],\n",
              "         [12.2279],\n",
              "         [12.2339],\n",
              "         [12.2398],\n",
              "         [12.2458],\n",
              "         [12.2518],\n",
              "         [12.2577],\n",
              "         [12.2637]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7Yre310E4EW",
        "outputId": "39fe37e4-d91c-4df4-9850-94ac33bac4a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([40, 1]), torch.Size([10, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "DcFIrgdQFPw0",
        "outputId": "291b11d9-037a-4087-87bd-24b9c38efe3a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a PyTorch linear model"
      ],
      "metadata": {
        "id": "85dYnfZEFji7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a linear model by subclassing nn.Module\n",
        "class LinearRegressionModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Use nn.Linear() for creating the model parameters\n",
        "    self.linear_layer = nn.Linear(in_features=1,\n",
        "                                  out_features=1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.linear_layer(x)\n",
        "\n",
        "# Set the manual seed\n",
        "torch.manual_seed(42)\n",
        "model_1 = LinearRegressionModelV2()\n",
        "model_1, model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYdwFnbjFVl9",
        "outputId": "b490c21d-2849-46e0-d252-1513df301457"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(LinearRegressionModelV2(\n",
              "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
              " ),\n",
              " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
              "              ('linear_layer.bias', tensor([0.8300]))]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the model's current device\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aSFud96b7cX",
        "outputId": "57657cc2-5935-4197-c0fd-541b68ca433c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the model to use the target device\n",
        "model_1.to(device)\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNQfytmRGk7U",
        "outputId": "bdcc7c54-31a1-4876-a97e-de027d509b44"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Training\n",
        "For training we need :\n",
        "* Loss function\n",
        "* Optimizer\n",
        "* Training Loop\n",
        "* Testing Loop"
      ],
      "metadata": {
        "id": "2IEgCzmrcVfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function\n",
        "loss_fn = nn.L1Loss() # same as MAE\n",
        "\n",
        "# Setup our optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "AhIA7ymjcL_B"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's write a training loop\n",
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 20000\n",
        "\n",
        "# device agnostic code for data\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "for  epoch in range(epochs):\n",
        "  model_1.train()\n",
        "\n",
        "  # 1. Forward Pass\n",
        "  y_pred = model_1(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    y_test_preds = model_1(X_test)\n",
        "    test_loss = loss_fn(y_test_preds, y_test)\n",
        "\n",
        "  # Print out what's happening\n",
        "  print(f\"Train Loss:{loss} | Test Loss:{test_loss}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBlAywRc7ke",
        "outputId": "3596044c-adaa-441c-cef5-d6b887c95e11"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n",
            "Train Loss:0.004435420036315918 | Test Loss:0.005904579069465399\n",
            "Train Loss:0.0070858001708984375 | Test Loss:0.007566547486931086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2IIVikwerSI",
        "outputId": "774847f2-0d43-4a98-e6cf-15ba3e370fb2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[0.3046]], device='cuda:0')),\n",
              "             ('linear_layer.bias', tensor([12.2120], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train.cpu(),\n",
        "                     train_labels=y_train.cpu(),\n",
        "                     test_data=X_test.cpu(),\n",
        "                     test_labels=y_test.cpu(),\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10,7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing Data\")\n",
        "\n",
        "  # Are there predictions ?\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions if they exist\n",
        "    plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\":14})"
      ],
      "metadata": {
        "id": "JiQqB81HgBai"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = model_1(X_test)\n",
        "\n",
        "plot_predictions(predictions=y_preds.cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7g6__Xs-fPEA",
        "outputId": "f01cad44-a9c6-4adb-c67b-3bcd36eaa801"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Saving and Loading a trained model"
      ],
      "metadata": {
        "id": "Fc1XJz2mhtYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create model directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to:{MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_1.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTyuCw9ffans",
        "outputId": "7124f856-a0dc-47cf-af04-3c2e254b7658"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to:models/01_pytorch_workflow_model_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bonuM9_dizOv",
        "outputId": "f8a66552-1c3b-43d2-8bda-39355e0000a6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[0.3046]], device='cuda:0')),\n",
              "             ('linear_layer.bias', tensor([12.2120], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a PyTorch Model\n",
        "\n",
        "# Create a new instance of linear regression model V2\n",
        "loaded_model_1 = LinearRegressionModelV2()\n",
        "\n",
        "# load the saved model to model_1 state_dict\n",
        "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Put the loaded model to device\n",
        "loaded_model_1.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUEiYmNii1xQ",
        "outputId": "516eeaa5-8d4a-4171-9d6f-69b24a96b0fc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModelV2(\n",
              "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(loaded_model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKoZ1hQHjcTW",
        "outputId": "3001bb40-8596-42d8-bc82-c10375fb7b72"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model\n",
        "loaded_model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_1_preds = loaded_model_1(X_test)\n",
        "y_preds == loaded_model_1_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm6AwVWljhn5",
        "outputId": "8c9c4ded-872a-4889-c2cb-8086fcfd97bb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}